{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Management with Weave\n",
    "\n",
    "This notebook demonstrates the dataset management capabilities of the Weave framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from weave.datasets import (\n",
    "    DatasetLoader,\n",
    "    DatasetMerger,\n",
    "    HuggingFaceDataset,\n",
    "    StreamingDataset\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data from Various Sources\n",
    "\n",
    "Demonstrate different data loading capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load from CSV\n",
    "loader = DatasetLoader()\n",
    "csv_data = loader.load(\"path/to/data.csv\")\n",
    "\n",
    "# Load from Kaggle\n",
    "kaggle_data = loader.load(\"kaggle://username/dataset/file.csv\")\n",
    "\n",
    "# Load from SQL database\n",
    "sql_data = loader.load(\"sqlite:///path/to/database.db\")\n",
    "\n",
    "print(\"Data sources loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HuggingFace Dataset Integration\n",
    "\n",
    "Work with datasets from the HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load dataset from HuggingFace\n",
    "hf_dataset = HuggingFaceDataset({\n",
    "    \"split\": \"train\",\n",
    "    \"transforms\": {\n",
    "        \"text\": \"join_tokens\"\n",
    "    }\n",
    "})\n",
    "\n",
    "data = hf_dataset.load(\"owner/dataset-name\")\n",
    "print(f\"Features: {data.get_features()}\")\n",
    "print(f\"Available splits: {data.get_splits()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Large Datasets\n",
    "\n",
    "Handle large datasets efficiently using streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stream large CSV file\n",
    "with StreamingDataset({\"chunk_size\": 1000}) as stream:\n",
    "    stream.load(\"path/to/large_file.csv\")\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk in stream.iter_chunks():\n",
    "        print(f\"Processing chunk of size: {len(chunk)}\")\n",
    "        \n",
    "    # Take first n records\n",
    "    head = stream.take(5)\n",
    "    print(\"\\nFirst 5 records:\")\n",
    "    print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Synthetic and Real Data\n",
    "\n",
    "Combine synthetic data with real datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load real and synthetic data\n",
    "real_data = pd.DataFrame({\n",
    "    \"text\": [\"Example 1\", \"Example 2\"],\n",
    "    \"label\": [0, 1]\n",
    "})\n",
    "\n",
    "synthetic_data = pd.DataFrame({\n",
    "    \"text\": [\"Synthetic 1\", \"Synthetic 2\"],\n",
    "    \"label\": [1, 0]\n",
    "})\n",
    "\n",
    "# Create merger\n",
    "merger = DatasetMerger()\n",
    "\n",
    "# Try different merge strategies\n",
    "append_result = merger.merge(real_data, synthetic_data, strategy=\"append\")\n",
    "print(\"\\nAppend strategy:\")\n",
    "print(append_result)\n",
    "\n",
    "mix_result = merger.merge(real_data, synthetic_data, strategy=\"mix\", ratio=0.3)\n",
    "print(\"\\nMix strategy (30% synthetic):\")\n",
    "print(mix_result)\n",
    "\n",
    "# Analyze distributions\n",
    "analysis = merger.analyze_distribution(real_data, synthetic_data)\n",
    "print(\"\\nDistribution analysis:\")\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Preprocessing\n",
    "\n",
    "Apply preprocessing steps to datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure preprocessing\n",
    "loader = DatasetLoader({\n",
    "    \"fill_value\": 0,\n",
    "    \"drop_duplicates\": True,\n",
    "    \"type_conversions\": {\n",
    "        \"numeric_col\": \"float32\",\n",
    "        \"category_col\": \"category\"\n",
    "    }\n",
    "})\n",
    "\n",
    "# Load and preprocess\n",
    "data = loader.load(\"path/to/data.csv\").preprocess()\n",
    "\n",
    "# Split dataset\n",
    "splits = data.split(\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "for name, split in splits.items():\n",
    "    print(f\"{name}: {len(split)} records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
